{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WV-pRPOTrhJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RRRZW12mid1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py9fkuvxmtm1"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VhQo3BNnIv4"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUeZmqDvnSdM"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Lc_QLyo_OB"
      },
      "outputs": [],
      "source": [
        "# Display the number of unique values for each column\n",
        "print(\"Number of unique values per column:\")\n",
        "display(df.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSTztDy59Sle"
      },
      "outputs": [],
      "source": [
        "# Display the number of unique values for each column\n",
        "print(\"Number of unique values per column:\")\n",
        "display(df_test.nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZEh7__8rhV2"
      },
      "outputs": [],
      "source": [
        "# Drop specified columns\n",
        "df = df.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1)\n",
        "df_test = df_test.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1)\n",
        "print(\"DataFrames after dropping specified columns:\")\n",
        "print(\"\\nTraining DataFrame (df):\")\n",
        "display(df.head())\n",
        "print(\"\\nTest DataFrame (df_test):\")\n",
        "display(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si-84ikEnejz"
      },
      "outputs": [],
      "source": [
        "# Manually select numerical and ordinal columns\n",
        "numerical_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'EmployeeNumber', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
        "ordinal_cols = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics for numerical columns\n",
        "print(\"Summary statistics for numerical columns:\")\n",
        "display(df[numerical_cols].describe().T)"
      ],
      "metadata": {
        "id": "ffNlh9Sv7SKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbpHW07J7oCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80553b6a"
      },
      "source": [
        "# Calculate the mean of numerical columns grouped by 'Attrition'\n",
        "print(\"Mean of numerical columns grouped by Attrition:\")\n",
        "display(df[numerical_cols + ['Attrition']].groupby('Attrition')[numerical_cols].mean().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PRfoCiDveMm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Determine the number of rows and columns for the grid\n",
        "n_cols = 4  # You can adjust the number of columns\n",
        "n_rows = math.ceil(len(numerical_cols) / n_cols)\n",
        "\n",
        "# Create a figure and subplots\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5)) # Adjust figure size as needed\n",
        "\n",
        "# Flatten the axes array for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Create vertical boxplots for each numerical column in numerical_cols\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.boxplot(y=df[col], ax=axes[i])\n",
        "    axes[i].set_title(f'Boxplot of {col}')\n",
        "    axes[i].set_ylabel('') # Remove y-label for clarity\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e91ccc1d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create histograms for each numerical column\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(df[col], bins=30, edgecolor='black')\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "959778fe"
      },
      "outputs": [],
      "source": [
        "# Calculate the skewness of numerical columns\n",
        "skewed_cols = df[numerical_cols].skew()\n",
        "\n",
        "print(\"Skewness of numerical columns:\")\n",
        "display(skewed_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74f6ab7b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Define the columns to apply Yeo-Johnson transformation\n",
        "yeo_johnson_cols = ['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany', 'YearsSinceLastPromotion']\n",
        "\n",
        "# Apply Yeo-Johnson transformation to the specified columns\n",
        "yeo_johnson_transformer = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "df[yeo_johnson_cols] = yeo_johnson_transformer.fit_transform(df[yeo_johnson_cols])\n",
        "\n",
        "print(\"DataFrames after applying Yeo-Johnson transformation to specified columns:\")\n",
        "print(\"\\nTraining DataFrame (df):\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "478bd08e"
      },
      "outputs": [],
      "source": [
        "print(\"Skewness of numerical columns:\")\n",
        "display(df[numerical_cols].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea8e2cc"
      },
      "outputs": [],
      "source": [
        "# Separate features (X_train) and target variable (y_train)\n",
        "x_train = df.drop('Attrition', axis=1)\n",
        "y_train = df['Attrition']\n",
        "\n",
        "print(\"X_train (features):\")\n",
        "display(x_train)\n",
        "\n",
        "print(\"\\ny_train (target):\")\n",
        "display(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeteS9Ox_iX6"
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPm7-c8wv-yw"
      },
      "outputs": [],
      "source": [
        "import category_encoders as ce\n",
        "\n",
        "nominal_cols = x_train.select_dtypes(include='object').columns.tolist()\n",
        "if 'id' in nominal_cols:\n",
        "    nominal_cols.remove('id')\n",
        "\n",
        "# Gunakan OrdinalEncoder yang handle unknown categories\n",
        "encoder = ce.OrdinalEncoder(\n",
        "    cols=nominal_cols,\n",
        "    handle_unknown='value',  # Assign -1 untuk kategori baru\n",
        "    handle_missing='value'   # Assign -2 untuk missing values\n",
        ")\n",
        "\n",
        "# TRAIN: Fit dan transform\n",
        "x_train_encoded = encoder.fit_transform(x_train)\n",
        "\n",
        "# TEST: Transform saja\n",
        "x_test_encoded = encoder.transform(df_test)\n",
        "\n",
        "print(\"DataFrames after Ordinal Encoding:\")\n",
        "print(\"\\nTraining DataFrame:\")\n",
        "display(x_train_encoded)\n",
        "print(\"\\nTest DataFrame:\")\n",
        "display(x_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac86b62c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Inisialisasi scaler\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "# Tentukan kolom yang akan di-scale\n",
        "cols_to_scale = [col for col in x_train_encoded.columns if col not in ['id']]\n",
        "\n",
        "# Pastikan kolom yang sama ada di df_test\n",
        "missing_cols = set(cols_to_scale) - set(x_test_encoded.columns)\n",
        "if missing_cols:\n",
        "    print(f\"Warning: Kolom {missing_cols} tidak ada di df_test\")\n",
        "    cols_to_scale = [col for col in cols_to_scale if col in x_test_encoded.columns]\n",
        "\n",
        "# TRAIN: Fit dan transform\n",
        "x_train_encoded[cols_to_scale] = robust_scaler.fit_transform(x_train_encoded[cols_to_scale])\n",
        "\n",
        "# TEST: Transform saja\n",
        "x_test_encoded[cols_to_scale] = robust_scaler.transform(x_test_encoded[cols_to_scale])\n",
        "\n",
        "print(\"DataFrames after scale:\")\n",
        "print(\"\\nTraining DataFrame:\")\n",
        "display(x_train_encoded)\n",
        "print(\"\\nTest DataFrame:\")\n",
        "display(x_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWgLNrfb6e4E"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Exclude the 'id' column and calculate the correlation matrix\n",
        "x_train_correlation = x_train_encoded.drop('id', axis=1).corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(x_train_correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Variables (Excluding ID)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60618a33"
      },
      "outputs": [],
      "source": [
        "# Find pairs of variables with correlation greater than or equal to 0.8\n",
        "high_correlation_pairs = x_train_correlation.unstack().sort_values(ascending=False)\n",
        "\n",
        "# Remove self-correlations and duplicate pairs\n",
        "high_correlation_pairs = high_correlation_pairs[(abs(high_correlation_pairs) >= 0.8) & (abs(high_correlation_pairs) <1)]\n",
        "\n",
        "print(\"Pairs of variables with correlation >= 0.8:\")\n",
        "display(high_correlation_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1385809"
      },
      "outputs": [],
      "source": [
        "# Drop the specified columns from the DataFrame\n",
        "cols_reduced = ['JobLevel', 'YearsInCurrentRole', 'YearsWithCurrManager']\n",
        "\n",
        "# Exclude the 'id' column and calculate the correlation matrix for the reduced DataFrame\n",
        "x_train_reduced = x_train_encoded.drop(columns=cols_reduced)\n",
        "x_test_reduced = x_test_encoded.drop(columns=cols_reduced)\n",
        "\n",
        "x_train_correlation_reduced = x_train_reduced.drop('id', axis=1).corr()\n",
        "\n",
        "# Create the heatmap for the reduced DataFrame\n",
        "plt.figure(figsize=(20, 15))\n",
        "sns.heatmap(x_train_correlation_reduced, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Variables (Excluding ID)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyJwDlLoTBdD"
      },
      "outputs": [],
      "source": [
        "print(\"Data x Train\")\n",
        "print(x_train_reduced.info())\n",
        "print(\"\\nData y Train\")\n",
        "print(y_train.info())\n",
        "print(\"\\nData Test\")\n",
        "print(x_test_reduced.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9diDzGjMPJjP"
      },
      "outputs": [],
      "source": [
        "print(\"Data x Train\")\n",
        "display(x_train_reduced)\n",
        "print(\"\\nData Train\")\n",
        "display(y_train)\n",
        "print(\"\\nData Test\")\n",
        "display(x_test_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ1xDQhY78sK"
      },
      "source": [
        "# Code coba coba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gofjkdmI78sN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Drop kolom 'id' dari data training dan testing\n",
        "x_train_final = x_train_reduced.drop('id', axis=1)\n",
        "x_test_final = x_test_reduced.drop('id', axis=1)\n",
        "\n",
        "# Definisikan dictionary model\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"Support Vector Machine\": SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Latih dan evaluasi model di data training\n",
        "roc_auc_scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(x_train_final, y_train)\n",
        "\n",
        "    # Prediksi probabilitas di data training (karena y_test tidak ada)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_proba_train = model.predict_proba(x_train_final)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba_train = model.decision_function(x_train_final)\n",
        "\n",
        "    # Hitung ROC AUC di training set\n",
        "    roc_auc = roc_auc_score(y_train, y_pred_proba_train)\n",
        "    roc_auc_scores[name] = roc_auc\n",
        "    print(f\"{name} ROC AUC (train): {roc_auc:.4f}\")\n",
        "\n",
        "# Tampilkan skor ROC AUC\n",
        "print(\"\\nROC AUC Scores:\")\n",
        "for model_name, score in roc_auc_scores.items():\n",
        "    print(f\"{model_name}: {score:.4f}\")\n",
        "\n",
        "# Model terbaik berdasarkan ROC AUC\n",
        "best_model_name = max(roc_auc_scores, key=roc_auc_scores.get)\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nBest Model: {best_model_name} (ROC AUC = {roc_auc_scores[best_model_name]:.4f})\")\n",
        "\n",
        "# Prediksi probabilitas untuk data test\n",
        "y_test_pred = best_model.predict_proba(x_test_final)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUTBHC5G78sN"
      },
      "source": [
        "# Revisi disini - gajadi direvisi, pakenya yg coba coba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903c2845"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Drop the 'id' column from the training and test data before training\n",
        "x_train_final = x_train_reduced.drop('id', axis=1)\n",
        "x_test_final = x_test_reduced.drop('id', axis=1)\n",
        "\n",
        "# Split the original training data into training and validation sets\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    x_train_final, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "\n",
        "# Define a dictionary of models to evaluate\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"Support Vector Machine\": SVC(probability=True, random_state=42) # probability=True needed for roc_auc_score\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "roc_auc_scores = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(x_train_split, y_train_split)\n",
        "\n",
        "    # Predict probabilities for ROC AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_proba = model.predict_proba(x_val_split)[:, 1]\n",
        "    else: # For models like SVC without predict_proba by default\n",
        "        y_pred_proba = model.decision_function(x_val_split)\n",
        "\n",
        "\n",
        "    # Calculate ROC AUC, handling potential errors\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_val_split, y_pred_proba)\n",
        "        roc_auc_scores[name] = roc_auc\n",
        "        print(f\"{name} ROC AUC: {roc_auc:.4f}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not calculate ROC AUC for {name}: {e}\")\n",
        "        roc_auc_scores[name] = None\n",
        "\n",
        "\n",
        "# Display the ROC AUC scores\n",
        "print(\"\\nROC AUC Scores:\")\n",
        "display(roc_auc_scores)\n",
        "\n",
        "# Find the model with the highest ROC AUC score\n",
        "best_model_name = None\n",
        "best_roc_auc = -1\n",
        "for name, roc_auc in roc_auc_scores.items():\n",
        "    if roc_auc is not None and roc_auc > best_roc_auc:\n",
        "        best_roc_auc = roc_auc\n",
        "        best_model_name = name\n",
        "\n",
        "print(f\"\\nBest performing model based on ROC AUC: {best_model_name} with ROC AUC = {best_roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SM7Bl8p78sO"
      },
      "source": [
        "# Coba Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_ChyjWa78sO"
      },
      "outputs": [],
      "source": [
        "# === 2️⃣ Inisialisasi dan latih model Random Forest ===\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,       # jumlah pohon lebih banyak = lebih stabil\n",
        "    max_depth=None,         # biarkan model cari kedalaman optimal\n",
        "    random_state=42,\n",
        "    n_jobs=-1               # gunakan semua core CPU\n",
        ")\n",
        "\n",
        "rf_model.fit(x_train_final, y_train)\n",
        "\n",
        "# === 3️⃣ Prediksi probabilitas di data test ===\n",
        "y_test_pred = rf_model.predict_proba(x_test_final)[:, 1]\n",
        "\n",
        "# === 4️⃣ Buat DataFrame hasil prediksi ===\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": x_test_reduced[\"id\"],\n",
        "    \"prediction\": y_test_pred\n",
        "})\n",
        "\n",
        "# === 5️⃣ Simpan ke file CSV untuk submission Kaggle ===\n",
        "submission_df.to_csv(\"submission_random_forest.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a00fd12"
      },
      "outputs": [],
      "source": [
        "# Train the best performing model (Logistic Regression) on the entire training data\n",
        "best_model = LogisticRegression(random_state=42)\n",
        "best_model.fit(x_train_final, y_train)\n",
        "\n",
        "# Predict probabilities on the test data\n",
        "test_predictions_proba = best_model.predict_proba(x_test_final)[:, 1]\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission_df = pd.DataFrame({'id': df_test['id'], 'Attrition': test_predictions_proba})\n",
        "\n",
        "print(\"Submission DataFrame:\")\n",
        "display(submission_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnbZGpsUvgDA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081e4c36"
      },
      "source": [
        "# Task\n",
        "Perform hyperparameter tuning on the previously evaluated models to optimize their performance, focusing on improving the ROC AUC score. Compare the performance of the tuned models and select the best one. Finally, train the best model on the entire training data and make predictions on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c2b8314"
      },
      "source": [
        "## Select model for tuning\n",
        "\n",
        "### Subtask:\n",
        "Choose one or more of the previously evaluated models for hyperparameter tuning (e.g., Logistic Regression, Random Forest, Gradient Boosting, SVM, XGBoost).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d183c0e7"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the ROC AUC scores and identify the best performing model to select for hyperparameter tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAq-7jxX78sP"
      },
      "source": [
        "# 1st Tune - Randomized Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "WXyr7eq878sP",
        "outputId": "7e1bd9bf-b078-4410-e6b1-faa47893db9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3349146631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrf_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "x_train_final = x_train_reduced.drop('id', axis=1)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='roc_auc',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_random_search.fit(x_train_final, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rf_random_search.best_params_)\n",
        "print(\"Best ROC AUC (CV):\", rf_random_search.best_score_)\n",
        "\n",
        "best_rf_model = rf_random_search.best_estimator_\n",
        "\n",
        "x_test_final = x_test_reduced.drop('id', axis=1)\n",
        "y_test_pred = best_rf_model.predict_proba(x_test_final)[:, 1]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": x_test_reduced[\"id\"],\n",
        "    \"prediction\": y_test_pred\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"submission_rf_tuned.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDndyFRe78sQ"
      },
      "source": [
        "# 2nd Tune - GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgOwZCBs78sQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_grid_search.fit(x_train_final, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rf_grid_search.best_params_)\n",
        "print(\"Best ROC AUC (CV):\", rf_grid_search.best_score_)\n",
        "\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "x_test_final = x_test_reduced.drop('id', axis=1)\n",
        "y_test_pred = best_rf_model.predict_proba(x_test_final)[:, 1]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": x_test_reduced[\"id\"],\n",
        "    \"prediction\": y_test_pred\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"submission_rf_gridsearch.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_c00cIx78sQ"
      },
      "source": [
        "# 3rd Tune - XGBoost + RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTG2dWv478se"
      },
      "outputs": [],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHIaChGp78se"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'reg_lambda': [0.5, 1, 1.5, 2],\n",
        "    'reg_alpha': [0, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='roc_auc',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_random_search.fit(x_train_final, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", xgb_random_search.best_params_)\n",
        "print(\"Best ROC AUC (CV):\", xgb_random_search.best_score_)\n",
        "\n",
        "best_xgb_model = xgb_random_search.best_estimator_\n",
        "\n",
        "y_test_pred = best_xgb_model.predict_proba(x_test_final)[:, 1]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": x_test_reduced[\"id\"],\n",
        "    \"prediction\": y_test_pred\n",
        "})\n",
        "\n",
        "submission_df.to_csv(\"submission_xgb_randomsearch.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terbaik 1\n"
      ],
      "metadata": {
        "id": "A9jnD0apjHJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ FIXED VERSION: ATTRITION STACKING ENSEMBLE (FINAL)\n",
        "# =====================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from category_encoders import TargetEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# === LOAD DATA ===\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "id_col = \"id\"\n",
        "target_col = \"Attrition\"\n",
        "\n",
        "X = train.drop([id_col, target_col], axis=1)\n",
        "y = train[target_col]\n",
        "X_test = test.drop(id_col, axis=1)\n",
        "\n",
        "# === SPLIT KATEGORIKAL & NUMERIK ===\n",
        "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
        "\n",
        "# === TARGET ENCODING untuk categorical ===\n",
        "te = TargetEncoder(cols=cat_cols)\n",
        "X[cat_cols] = te.fit_transform(X[cat_cols], y)\n",
        "X_test[cat_cols] = te.transform(X_test[cat_cols])\n",
        "\n",
        "# === SCALING untuk numerical ===\n",
        "scaler = RobustScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "# === HANDLE IMBALANCE dengan SMOTE ===\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# === BASE MODELS ===\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=6,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    scale_pos_weight=2,\n",
        "    reg_lambda=1.2,\n",
        "    reg_alpha=0.2,\n",
        "    eval_metric=\"auc\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    num_leaves=40,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    metric='auc'\n",
        ")\n",
        "\n",
        "cat = CatBoostClassifier(\n",
        "    iterations=800,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=2,\n",
        "    eval_metric='AUC',\n",
        "    verbose=0,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "# === STACKING ENSEMBLE ===\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb),\n",
        "        ('lgbm', lgbm),\n",
        "        ('cat', cat)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=200),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# === CROSS-VALIDATION ===\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X_res))\n",
        "fold = 1\n",
        "\n",
        "for train_idx, val_idx in skf.split(X_res, y_res):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    X_tr, X_val = X_res.iloc[train_idx], X_res.iloc[val_idx]\n",
        "    y_tr, y_val = y_res.iloc[train_idx], y_res.iloc[val_idx]\n",
        "\n",
        "    stack_model.fit(X_tr, y_tr)\n",
        "    val_pred = stack_model.predict_proba(X_val)[:, 1]\n",
        "    oof_preds[val_idx] = val_pred\n",
        "\n",
        "    auc = roc_auc_score(y_val, val_pred)\n",
        "    print(f\"Fold {fold} AUC: {auc:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "# === Overall AUC ===\n",
        "overall_auc = roc_auc_score(y_res, oof_preds)\n",
        "print(f\"\\n✅ Overall ROC-AUC: {overall_auc:.4f}\")\n",
        "\n",
        "# === FIT FULL MODEL & PREDICT TEST ===\n",
        "stack_model.fit(X_res, y_res)\n",
        "final_pred = stack_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# === SUBMISSION ===\n",
        "submission = pd.DataFrame({\n",
        "    id_col: test[id_col],\n",
        "    \"prediction\": final_pred\n",
        "})\n",
        "submission.to_csv(\"submission_attrition_final.csv\", index=False)\n",
        "\n",
        "print(\"\\n🎯 File 'submission_attrition_final.csv' berhasil dibuat tanpa error!\")\n"
      ],
      "metadata": {
        "id": "KyVJiDTEgsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f04b8cc"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "kzITz8Z96Nu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt\n"
      ],
      "metadata": {
        "id": "_5C4tLYv6343"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"requirements.txt\")\n"
      ],
      "metadata": {
        "id": "0uRsoKLj7E0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "26kLC-EPfkbx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73e248f2"
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score\n",
        "\n",
        "# === 1. RandomForest (GridSearchCV) ===\n",
        "rf_model = best_rf_model\n",
        "rf_val_pred_prob = cross_val_predict(rf_model, x_train_final, y_train, cv=3, method='predict_proba')[:, 1]\n",
        "rf_val_pred = (rf_val_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "rf_roc_auc = roc_auc_score(y_train, rf_val_pred_prob)\n",
        "rf_accuracy = accuracy_score(y_train, rf_val_pred)\n",
        "rf_recall = recall_score(y_train, rf_val_pred)\n",
        "\n",
        "# === 2. XGBoost (RandomizedSearchCV) ===\n",
        "xgb_model = best_xgb_model\n",
        "xgb_val_pred_prob = cross_val_predict(xgb_model, x_train_final, y_train, cv=3, method='predict_proba')[:, 1]\n",
        "xgb_val_pred = (xgb_val_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "xgb_roc_auc = roc_auc_score(y_train, xgb_val_pred_prob)\n",
        "xgb_accuracy = accuracy_score(y_train, xgb_val_pred)\n",
        "xgb_recall = recall_score(y_train, xgb_val_pred)\n",
        "\n",
        "# === 3. Stacking Ensemble ===\n",
        "stack_val_pred_prob = oof_preds  # sudah dihasilkan saat StratifiedKFold\n",
        "stack_val_pred = (stack_val_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "stack_roc_auc = roc_auc_score(y_res, stack_val_pred_prob)\n",
        "stack_accuracy = accuracy_score(y_res, stack_val_pred)\n",
        "stack_recall = recall_score(y_res, stack_val_pred)\n",
        "\n",
        "# === Buat dataframe perbandingan ===\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"Model\": [\"RandomForest (GridSearchCV)\", \"XGBoost (RandomizedSearchCV)\", \"Stacking Ensemble\"],\n",
        "    \"ROC AUC\": [rf_roc_auc, xgb_roc_auc, stack_roc_auc],\n",
        "    \"Accuracy\": [rf_accuracy, xgb_accuracy, stack_accuracy],\n",
        "    \"Recall\": [rf_recall, xgb_recall, stack_recall]\n",
        "})\n",
        "\n",
        "print(comparison_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Threshold 0.5 untuk konversi probabilitas menjadi label\n",
        "oof_pred_labels = (oof_preds >= 0.5).astype(int)\n",
        "\n",
        "# Buat classification report\n",
        "report_dict = classification_report(y_res, oof_pred_labels, target_names=['No Attrition', 'Attrition'], output_dict=True)\n",
        "\n",
        "# Konversi ke DataFrame\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Round angka desimal agar rapi\n",
        "report_df.iloc[:, :-1] = report_df.iloc[:, :-1].round(4)\n",
        "\n",
        "# Cetak dataframe\n",
        "print(report_df)\n"
      ],
      "metadata": {
        "id": "iZJeolKjUKJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
        "from scipy import stats\n",
        "\n",
        "# --- Hitung metrik ---\n",
        "brier = brier_score_loss(y_res, oof_preds)\n",
        "\n",
        "roc_auc = roc_auc_score(y_res, oof_preds)\n",
        "gini = 2 * roc_auc - 1\n",
        "\n",
        "preds_positive = oof_preds[y_res == 1]  # Attrition\n",
        "preds_negative = oof_preds[y_res == 0]  # No Attrition\n",
        "ks_stat, _ = stats.ks_2samp(preds_positive, preds_negative)\n",
        "\n",
        "# --- Buat DataFrame ---\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Metrik\": [\"Brier Score\", \"Gini Coefficient\", \"KS Statistic\"],\n",
        "    \"Stacking Ensemble\": [brier, gini, ks_stat]\n",
        "})\n",
        "\n",
        "# Round agar rapi\n",
        "metrics_df[\"Stacking Ensemble\"] = metrics_df[\"Stacking Ensemble\"].round(6)\n",
        "\n",
        "# Cetak tabel\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "id": "k1z3rv7PycVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Threshold 0.5 untuk konversi probabilitas menjadi label\n",
        "oof_pred_labels = (oof_preds >= 0.5).astype(int)\n",
        "\n",
        "# Buat confusion matrix\n",
        "cm = confusion_matrix(y_res, oof_pred_labels)\n",
        "\n",
        "# Cetak matrix\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualisasi dengan heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Attrition','Attrition'], yticklabels=['No Attrition','Attrition'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Stacking Ensemble')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ULLO7bQL20rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Hitung ROC curve ---\n",
        "fpr, tpr, thresholds = roc_curve(y_res, oof_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# --- Plot ROC curve ---\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Stacking Ensemble')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nJVRJzJa3D20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- ROC curve & AUC ---\n",
        "fpr, tpr, thresholds = roc_curve(y_res, oof_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# --- Plot ROC curve ---\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Stacking Ensemble (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC-AUC Curve - Stacking Ensemble')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Cetak nilai AUC ---\n",
        "print(f\"ROC-AUC Stacking Ensemble: {roc_auc:.6f}\")\n"
      ],
      "metadata": {
        "id": "VOerPpUl3MET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Probabilitas prediksi ---\n",
        "y_scores = oof_preds  # prediksi probabilitas dari Stacking Ensemble\n",
        "\n",
        "# --------------------------\n",
        "# ROC-AUC Curve\n",
        "# --------------------------\n",
        "fpr, tpr, _ = roc_curve(y_res, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Stacking Ensemble')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "\n",
        "# --------------------------\n",
        "# Precision-Recall Curve\n",
        "# --------------------------\n",
        "precision, recall, _ = precision_recall_curve(y_res, y_scores)\n",
        "pr_auc = average_precision_score(y_res, y_scores)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(recall, precision, color='green', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Stacking Ensemble')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Cetak nilai AUC ---\n",
        "print(f\"ROC-AUC Stacking Ensemble: {roc_auc:.6f}\")\n",
        "print(f\"PR-AUC Stacking Ensemble: {pr_auc:.6f}\")\n"
      ],
      "metadata": {
        "id": "mdbMKWGT3Vwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Buat DataFrame probabilitas dan label asli\n",
        "df_probs = pd.DataFrame({\n",
        "    \"prob_attrition\": oof_preds,  # probabilitas prediksi Attrition\n",
        "    \"actual\": y_res               # label asli (0=No Attrition, 1=Attrition)\n",
        "})\n",
        "\n",
        "# --- Analisis Distribusi Probabilitas ---\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df_probs[df_probs[\"actual\"]==0][\"prob_attrition\"],\n",
        "             color='blue', label='No Attrition', kde=True, bins=30, stat='density', alpha=0.6)\n",
        "sns.histplot(df_probs[df_probs[\"actual\"]==1][\"prob_attrition\"],\n",
        "             color='red', label='Attrition', kde=True, bins=30, stat='density', alpha=0.6)\n",
        "plt.title('Distribusi Probabilitas Prediksi Attrition - Stacking Ensemble')\n",
        "plt.xlabel('Probabilitas Prediksi Attrition')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-lZQBnF43ppC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Buat DataFrame probabilitas dan label asli\n",
        "df_probs = pd.DataFrame({\n",
        "    \"prob_attrition\": oof_preds,  # probabilitas prediksi Attrition\n",
        "    \"actual\": y_res               # label asli (0=No Attrition, 1=Attrition)\n",
        "})\n",
        "\n",
        "# --- Plot KDE distribusi probabilitas ---\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(df_probs[df_probs[\"actual\"]==0][\"prob_attrition\"],\n",
        "            shade=True, color='steelblue', label='No Attrition', alpha=0.6)\n",
        "sns.kdeplot(df_probs[df_probs[\"actual\"]==1][\"prob_attrition\"],\n",
        "            shade=True, color='orange', label='Attrition', alpha=0.6)\n",
        "\n",
        "plt.title('Distribusi Probabilitas Prediksi Attrition - Stacking Ensemble')\n",
        "plt.xlabel('Predicted probabilities')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZgooFwrv9qV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Buat DataFrame probabilitas dan label asli\n",
        "df_probs = pd.DataFrame({\n",
        "    \"prob_attrition\": oof_preds,  # probabilitas prediksi Attrition\n",
        "    \"actual\": y_res               # label asli (0=No Attrition, 1=Attrition)\n",
        "})\n",
        "\n",
        "# --- Bagi ke dalam decile ---\n",
        "df_probs['decile'] = pd.qcut(df_probs['prob_attrition'], 10, labels=False) + 1  # 1 sampai 10\n",
        "\n",
        "# --- Hitung churn rate per decile ---\n",
        "decile_summary = df_probs.groupby('decile')['actual'].mean().reset_index()\n",
        "decile_summary.rename(columns={'actual': 'attrition_rate'}, inplace=True)\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(decile_summary['decile'], decile_summary['attrition_rate'], color='navy', alpha=0.8)\n",
        "plt.title('Probability Scores Ordering - attrition rate per Decile')\n",
        "plt.xlabel('Decile (1 = Lowest Prob, 10 = Highest Prob)')\n",
        "plt.ylabel('atrrition Rate')\n",
        "plt.xticks(decile_summary['decile'])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uttyunZv900P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === TAMBAHKAN LIBRARY UNTUK INTERPRETASI ===\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "import shap\n",
        "\n",
        "# Anda sudah fit model di baris sebelumnya:\n",
        "# stack_model.fit(X_res, y_res)\n",
        "print(\"\\n=== INTERPRETASI MODEL FINAL ===\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 1: Bobot Final Estimator (Logistic Regression)\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n--- Level 1: Bobot Base Model ---\")\n",
        "\n",
        "try:\n",
        "    # Mengakses final_estimator_ yang sudah di-fit\n",
        "    final_estimator = stack_model.final_estimator_\n",
        "\n",
        "    # Mendapatkan koefisien (bobot)\n",
        "    # .coef_[0] karena ini adalah klasifikasi biner\n",
        "    weights = final_estimator.coef_[0]\n",
        "\n",
        "    # Mendapatkan nama base models\n",
        "    model_names = [name for name, _ in stack_model.estimators]\n",
        "\n",
        "    print(\"Bobot yang diberikan Logistic Regression ke tiap base model:\")\n",
        "    for name, weight in zip(model_names, weights):\n",
        "        print(f\"-> {name}: {weight:.4f}\")\n",
        "\n",
        "    # Visualisasi Sederhana\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=model_names, y=weights, palette=\"viridis\")\n",
        "    plt.title(\"Bobot Base Model di Final Estimator (Logistic Regression)\")\n",
        "    plt.ylabel(\"Koefisien (Bobot)\")\n",
        "    plt.xlabel(\"Base Model\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Tidak bisa mengambil bobot final estimator: {e}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 2: Permutation Importance (Fitur Paling Penting)\n",
        "# -----------------------------------------------------------------\n",
        "# CATATAN: Idealnya, ini dijalankan pada validation set terpisah.\n",
        "# Karena kita fit pada semua data (X_res), kita akan hitung\n",
        "# importance pada data training itu, tapi ini bisa sedikit bias (overfit).\n",
        "print(\"\\n--- Level 2: Permutation Importance (Fitur Global) ---\")\n",
        "print(\"Menghitung Permutation Importance... (Mungkin perlu beberapa saat)\")\n",
        "\n",
        "# Kita hitung importance pada data X_res (data training)\n",
        "perm_importance = permutation_importance(\n",
        "    stack_model,\n",
        "    X_res,\n",
        "    y_res,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Dapatkan nama fitur\n",
        "feature_names = X.columns.tolist()\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "\n",
        "# Buat DataFrame untuk visualisasi\n",
        "perm_df = pd.DataFrame(\n",
        "    data={\n",
        "        'feature': np.array(feature_names)[sorted_idx],\n",
        "        'importance_mean': perm_importance.importances_mean[sorted_idx],\n",
        "    }\n",
        ").sort_values(by='importance_mean', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Fitur Paling Berpengaruh (Permutation Importance):\")\n",
        "print(perm_df.head(15))\n",
        "\n",
        "# Visualisasi Permutation Importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(\n",
        "    data=perm_df.head(15),\n",
        "    x='importance_mean',\n",
        "    y='feature',\n",
        "    palette='rocket'\n",
        ")\n",
        "plt.title(\"Top 15 Feature Importance (Permutation) untuk Stacked Model\")\n",
        "plt.xlabel(\"Penurunan Rata-rata Skor AUC\")\n",
        "plt.ylabel(\"Fitur\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 2 (Alternatif): SHAP (Proxy Method)\n",
        "# -----------------------------------------------------------------\n",
        "# Ini adalah metode \"proxy\": kita melihat SHAP dari masing-masing\n",
        "# base model yang sudah di-fit di dalam stack.\n",
        "print(\"\\n--- Level 2 (Alternatif): SHAP Summary per Base Model ---\")\n",
        "shap.initjs() # Inisialisasi javascript untuk plotting di notebook\n",
        "\n",
        "# Kita ambil sampel data untuk background (agar SHAP lebih cepat)\n",
        "# Gunakan X_res karena model di-fit di sana\n",
        "background_data = shap.utils.sample(X_res, 100)\n",
        "\n",
        "# Loop ketiga base model yang sudah di-fit di dalam stack\n",
        "for model_name, _ in stack_model.estimators:\n",
        "    try:\n",
        "        print(f\"\\nMenganalisis SHAP untuk: {model_name}\")\n",
        "\n",
        "        # Mengakses model yang sudah di-fit dari stack\n",
        "        model_in_stack = stack_model.named_estimators_[model_name]\n",
        "\n",
        "        # Buat TreeExplainer\n",
        "        explainer = shap.TreeExplainer(model_in_stack, background_data)\n",
        "\n",
        "        # Hitung SHAP values (bisa ambil sampel lebih kecil dari X_res agar cepat)\n",
        "        shap_values = explainer(X_res.sample(1000, random_state=1))\n",
        "\n",
        "        # Plot summary\n",
        "        # [:, :, 1] untuk mengambil SHAP values untuk kelas positif (Attrition=1)\n",
        "        shap.summary_plot(shap_values.values[:,:,1], X_res.sample(1000, random_state=1),\n",
        "                          show=False, plot_size=(10, 6))\n",
        "        plt.title(f\"SHAP Summary Plot untuk {model_name} (Class 1)\")\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membuat SHAP untuk {model_name}: {e}\")\n",
        "        print(\"Catatan: Beberapa model mungkin butuh perlakuan khusus untuk SHAP.\")\n",
        "\n",
        "\n",
        "print(\"\\n✅ Interpretasi model selesai.\")\n",
        "# Lanjutkan ke prediksi test\n",
        "# final_pred = stack_model.predict_proba(X_test)[:, 1]\n",
        "# ..."
      ],
      "metadata": {
        "id": "4zK0gu8a_dAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === TAMBAHKAN LIBRARY UNTUK INTERPRETASI ===\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import permutation_importance\n",
        "import shap\n",
        "import pandas as pd # Pastikan pandas diimpor jika belum\n",
        "\n",
        "# Anda sudah fit model di baris sebelumnya:\n",
        "# stack_model.fit(X_res, y_res)\n",
        "print(\"\\n=== INTERPRETASI MODEL FINAL ===\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 1: Bobot Final Estimator (Logistic Regression)\n",
        "# -----------------------------------------------------------------\n",
        "print(\"\\n--- Level 1: Bobot Base Model ---\")\n",
        "\n",
        "try:\n",
        "    # Mengakses final_estimator_ yang sudah di-fit\n",
        "    final_estimator = stack_model.final_estimator_\n",
        "\n",
        "    # Mendapatkan koefisien (bobot)\n",
        "    # .coef_[0] karena ini adalah klasifikasi biner\n",
        "    weights = final_estimator.coef_[0]\n",
        "\n",
        "    # Mendapatkan nama base models\n",
        "    model_names = [name for name, _ in stack_model.estimators]\n",
        "\n",
        "    print(\"Bobot yang diberikan Logistic Regression ke tiap base model:\")\n",
        "    for name, weight in zip(model_names, weights):\n",
        "        print(f\"-> {name}: {weight:.4f}\")\n",
        "\n",
        "    # Visualisasi Sederhana\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=model_names, y=weights, palette=\"viridis\")\n",
        "    plt.title(\"Bobot Base Model di Final Estimator (Logistic Regression)\")\n",
        "    plt.ylabel(\"Koefisien (Bobot)\")\n",
        "    plt.xlabel(\"Base Model\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Tidak bisa mengambil bobot final estimator: {e}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 2: Permutation Importance (Fitur Paling Penting)\n",
        "# -----------------------------------------------------------------\n",
        "# CATATAN: Idealnya, ini dijalankan pada validation set terpisah.\n",
        "# Karena kita fit pada semua data (X_res), kita akan hitung\n",
        "# importance pada data training itu, tapi ini bisa sedikit bias (overfit).\n",
        "print(\"\\n--- Level 2: Permutation Importance (Fitur Global) ---\")\n",
        "print(\"Menghitung Permutation Importance... (Mungkin perlu beberapa saat)\")\n",
        "\n",
        "# Kita hitung importance pada data X_res (data training)\n",
        "perm_importance = permutation_importance(\n",
        "    stack_model,\n",
        "    X_res,\n",
        "    y_res,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Dapatkan nama fitur\n",
        "feature_names = X.columns.tolist()\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "\n",
        "# Buat DataFrame untuk visualisasi\n",
        "perm_df = pd.DataFrame(\n",
        "    data={\n",
        "        'feature': np.array(feature_names)[sorted_idx],\n",
        "        'importance_mean': perm_importance.importances_mean[sorted_idx],\n",
        "    }\n",
        ").sort_values(by='importance_mean', ascending=False)\n",
        "\n",
        "# === PERUBAHAN DI SINI: Tampilkan Semua Fitur ===\n",
        "print(\"\\nSemua Fitur Paling Berpengaruh (Permutation Importance):\")\n",
        "# Atur agar Pandas menampilkan semua baris\n",
        "pd.set_option('display.max_rows', None)\n",
        "print(perm_df)\n",
        "pd.reset_option('display.max_rows') # Kembalikan ke default\n",
        "\n",
        "# === PERUBAHAN DI SINI: Visualisasi Semua Fitur ===\n",
        "# Tentukan tinggi plot secara dinamis berdasarkan jumlah fitur\n",
        "num_features = len(perm_df)\n",
        "# Atur tinggi minimal 8, dan tambahkan 0.5 inci per fitur\n",
        "plot_height = max(8, num_features * 0.5)\n",
        "\n",
        "plt.figure(figsize=(12, plot_height))\n",
        "sns.barplot(\n",
        "    data=perm_df,  # Gunakan perm_df (bukan .head(15))\n",
        "    x='importance_mean',\n",
        "    y='feature',\n",
        "    palette='rocket'\n",
        ")\n",
        "plt.title(\"Semua Feature Importance (Permutation) untuk Stacked Model\")\n",
        "plt.xlabel(\"Penurunan Rata-rata Skor AUC\")\n",
        "plt.ylabel(\"Fitur\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# INTERPRETASI LEVEL 2 (Alternatif): SHAP (Proxy Method)\n",
        "# -----------------------------------------------------------------\n",
        "# Ini adalah metode \"proxy\": kita melihat SHAP dari masing-masing\n",
        "# base model yang sudah di-fit di dalam stack.\n",
        "print(\"\\n--- Level 2 (Alternatif): SHAP Summary per Base Model ---\")\n",
        "shap.initjs() # Inisialisasi javascript untuk plotting di notebook\n",
        "\n",
        "# Kita ambil sampel data untuk background (agar SHAP lebih cepat)\n",
        "# Gunakan X_res karena model di-fit di sana\n",
        "background_data = shap.utils.sample(X_res, 100)\n",
        "\n",
        "# Loop ketiga base model yang sudah di-fit di dalam stack\n",
        "for model_name, _ in stack_model.estimators:\n",
        "    try:\n",
        "        print(f\"\\nMenganalisis SHAP untuk: {model_name}\")\n",
        "\n",
        "        # Mengakses model yang sudah di-fit dari stack\n",
        "        model_in_stack = stack_model.named_estimators_[model_name]\n",
        "\n",
        "        # Buat TreeExplainer\n",
        "        explainer = shap.TreeExplainer(model_in_stack, background_data)\n",
        "\n",
        "        # Hitung SHAP values (bisa ambil sampel lebih kecil dari X_res agar cepat)\n",
        "        shap_values = explainer(X_res.sample(1000, random_state=1))\n",
        "\n",
        "        # Plot summary\n",
        "        # [:, :, 1] untuk mengambil SHAP values untuk kelas positif (Attrition=1)\n",
        "        shap.summary_plot(shap_values.values[:,:,1], X_res.sample(1000, random_state=1),\n",
        "                          show=False, plot_size=(10, 6))\n",
        "        plt.title(f\"SHAP Summary Plot untuk {model_name} (Class 1)\")\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membuat SHAP untuk {model_name}: {e}\")\n",
        "        print(\"Catatan: Beberapa model mungkin butuh perlakuan khusus untuk SHAP.\")\n",
        "\n",
        "\n",
        "print(\"\\n✅ Interpretasi model selesai.\")\n",
        "# Lanjutkan ke prediksi test\n",
        "# final_pred = stack_model.predict_proba(X_test)[:, 1]\n",
        "# ..."
      ],
      "metadata": {
        "id": "WCAQv3rjDdej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ BLOK KODE INTERPRETASI SHAP\n",
        "# =====================================================\n",
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Pastikan variabel ini sudah ada dari script utama Anda ---\n",
        "# stack_model: Model StackingClassifier Anda yang sudah di-fit\n",
        "# X_res: DataFrame fitur Anda (misalnya, X_res atau X_val)\n",
        "# feature_names: Daftar nama kolom X_res\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== INTERPRETASI SHAP UNTUK SATU KARYAWAN ===\")\n",
        "\n",
        "# 1. Inisialisasi SHAP (diperlukan untuk plot di Jupyter/Colab)\n",
        "shap.initjs()\n",
        "\n",
        "# 2. Definisikan fungsi Sigmoid\n",
        "def sigmoid(x):\n",
        "    \"\"\"Mengubah log-odds menjadi probabilitas\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# 3. Definisikan fungsi prediksi untuk SHAP (mengembalikan log-odds)\n",
        "# SHAP KernelExplainer butuh output model dalam log-odds.\n",
        "def predict_log_odds(X):\n",
        "    # Pastikan X adalah DataFrame dengan nama fitur yang benar\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    else:\n",
        "        X_df = X\n",
        "\n",
        "    # Dapatkan probabilitas kelas 1 (Attrition)\n",
        "    probas = stack_model.predict_proba(X_df)[:, 1]\n",
        "\n",
        "    # Ubah probabilitas ke log-odds: log(p / (1-p))\n",
        "    # Tambahkan 'clipping' untuk menghindari log(0)\n",
        "    epsilon = 1e-15\n",
        "    probas_clipped = np.clip(probas, epsilon, 1 - epsilon)\n",
        "    log_odds = np.log(probas_clipped / (1 - probas_clipped))\n",
        "\n",
        "    return log_odds\n",
        "\n",
        "# 4. Buat SHAP Explainer\n",
        "print(\"Menyiapkan SHAP KernelExplainer...\")\n",
        "print(\"Catatan: Ini mungkin lambat. Kita gunakan 100 sampel data background.\")\n",
        "\n",
        "# Kita gunakan 'shap.sample' untuk membuat background data\n",
        "# 'background_data' adalah referensi untuk E[f(X)]\n",
        "background_data = shap.sample(X_res, 100)\n",
        "\n",
        "# Membuat KernelExplainer (model-agnostic)\n",
        "explainer = shap.KernelExplainer(predict_log_odds, background_data)\n",
        "\n",
        "# 5. Pilih satu karyawan untuk dianalisis (misal, indeks ke-10)\n",
        "karyawan_index = 10\n",
        "karyawan_data = X_res.iloc[[karyawan_index]]\n",
        "\n",
        "print(f\"\\nMenganalisis karyawan di indeks: {karyawan_index}\")\n",
        "\n",
        "# 6. Hitung SHAP values untuk karyawan tersebut\n",
        "# nsamples=50 (default 'auto') adalah jumlah sampling\n",
        "# untuk mengestimasi nilai SHAP.\n",
        "shap_values_karyawan = explainer.shap_values(karyawan_data, nsamples=50)\n",
        "\n",
        "# 7. Dapatkan Base Value (E[f(X)])\n",
        "# Ini adalah \"Base Value\" atau rata-rata prediksi log-odds\n",
        "base_value = explainer.expected_value\n",
        "print(f\"\\nBase Value (E[f(X)]) (rata-rata log-odds): {base_value:.4f}\")\n",
        "print(f\"Probabilitas Attrition rata-rata (Sigmoid dari Base Value): {sigmoid(base_value):.4f}\")\n",
        "\n",
        "# 8. Visualisasikan sebagai Waterfall Plot\n",
        "print(\"\\nMembuat Waterfall Plot (kontribusi fitur dalam log-odds)...\")\n",
        "#\n",
        "shap.waterfall_plot(shap.Explanation(\n",
        "    values=shap_values_karyawan[0], # Ambil SHAP values\n",
        "    base_values=base_value,        # Base E[f(X)]\n",
        "    data=karyawan_data.values[0],  # Nilai fitur asli\n",
        "    feature_names=feature_names    # Nama fitur\n",
        "))\n",
        "\n",
        "# 9. Verifikasi perhitungan manual (Log-odds -> Prob)\n",
        "print(\"\\n--- Verifikasi Manual ---\")\n",
        "final_log_odds = base_value + shap_values_karyawan[0].sum()\n",
        "prob_dari_shap = sigmoid(final_log_odds)\n",
        "prob_dari_model = stack_model.predict_proba(karyawan_data)[:, 1][0]\n",
        "\n",
        "print(f\"Log-odds Akhir f(x) (Base + SHAP): {final_log_odds:.4f}\")\n",
        "print(f\"Probabilitas Attrition (dari Sigmoid): {prob_dari_shap:.4f}\")\n",
        "print(f\"Probabilitas Attrition (dari Model.predict_proba): {prob_dari_model:.4f}\")\n",
        "\n",
        "if np.isclose(prob_dari_shap, prob_dari_model):\n",
        "    print(\"✅ Hasil perhitungan manual (Sigmoid) SESUAI dengan prediksi model!\")\n",
        "else:\n",
        "    print(\"⚠️ Hasil perhitungan manual BERBEDA. Cek proses Explainer.\")\n",
        "\n",
        "# 10. Tampilkan Force Plot\n",
        "print(\"\\nMembuat Force Plot (Visualisasi interaktif)...\")\n",
        "#\n",
        "display(shap.force_plot(\n",
        "    base_value,\n",
        "    shap_values_karyawan[0],\n",
        "    karyawan_data,\n",
        "    matplotlib=False # Gunakan versi Javascript (interaktif)\n",
        "))\n",
        "\n",
        "print(\"\\n✅ Interpretasi SHAP selesai.\")"
      ],
      "metadata": {
        "id": "L62aAOgNF8nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ BLOK KODE UNTUK SHAP GLOBAL FEATURE IMPORTANCE (Bar Plot)\n",
        "# =====================================================\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Pastikan numpy sudah diimpor\n",
        "\n",
        "# --- Pastikan variabel ini sudah ada dari script utama Anda ---\n",
        "# stack_model: Model StackingClassifier Anda yang sudah di-fit\n",
        "# X_res: DataFrame fitur Anda (misalnya, X_res atau X_val)\n",
        "# feature_names: Daftar nama kolom X_res\n",
        "# predict_log_odds: Fungsi yang mengembalikan prediksi log-odds (sudah kita definisikan)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== SHAP GLOBAL FEATURE IMPORTANCE (Bar Plot) ===\")\n",
        "\n",
        "# --- Langkah 1: Buat SHAP Explainer (Jika belum ada) ---\n",
        "# Jika Anda sudah membuat 'explainer' dari kode sebelumnya, Anda bisa langsung menggunakannya.\n",
        "# Jika tidak, kita perlu membuatnya lagi:\n",
        "try:\n",
        "    # Coba gunakan explainer yang sudah ada\n",
        "    if 'explainer' not in locals() or explainer is None:\n",
        "        raise NameError(\"explainer not found, creating a new one.\")\n",
        "    print(\"Menggunakan KernelExplainer yang sudah ada.\")\n",
        "except NameError:\n",
        "    print(\"Membuat SHAP KernelExplainer baru untuk Global Importance.\")\n",
        "    background_data_for_global = shap.sample(X_res, 100)\n",
        "    explainer = shap.KernelExplainer(predict_log_odds, background_data_for_global)\n",
        "\n",
        "\n",
        "# --- Langkah 2: Hitung SHAP Values untuk seluruh dataset ---\n",
        "# Menghitung SHAP values untuk seluruh dataset bisa sangat lambat,\n",
        "# terutama dengan KernelExplainer. Kita akan mengambil sampel data\n",
        "# yang lebih besar dari background_data tapi tidak semua X_res.\n",
        "print(\"Menghitung SHAP values untuk sampel data (ini mungkin perlu waktu)...\")\n",
        "sample_size_for_global_shap = min(2000, len(X_res)) # Batasi hingga 2000 sampel atau semua data\n",
        "shap_values_global = explainer.shap_values(X_res.sample(sample_size_for_global_shap, random_state=42))\n",
        "\n",
        "# --- Langkah 3: Membuat SHAP Summary Plot (Bar Plot) ---\n",
        "# Untuk plot seperti gambar Anda, kita perlu SHAP values untuk kelas positif (Attrition=1)\n",
        "# shap_values_global[0] jika kelas target 0, shap_values_global[1] jika kelas target 1\n",
        "# Asumsi kita tertarik pada kelas Attrition=1, yang biasanya indeks 1.\n",
        "# Pastikan 'predict_log_odds' sudah mengembalikan nilai yang relevan untuk kelas target positif.\n",
        "# KernelExplainer biasanya mengembalikan 1 array untuk output tunggal (log-odds).\n",
        "\n",
        "print(\"Membuat SHAP Summary Bar Plot...\")\n",
        "plt.figure(figsize=(10, 7)) # Sesuaikan ukuran figure jika banyak fitur\n",
        "shap.summary_plot(\n",
        "    shap_values_global, # SHAP values\n",
        "    X_res.sample(sample_size_for_global_shap, random_state=42), # Data asli untuk label fitur\n",
        "    plot_type=\"bar\", # Ini yang akan membuat bar plot\n",
        "    show=False,      # Jangan tampilkan langsung, kita atur judul sendiri\n",
        "    feature_names=feature_names, # Pastikan nama fitur benar\n",
        "    color_bar=False, # Tidak perlu color bar untuk bar plot\n",
        "    max_display=20 # Tampilkan 20 fitur teratas (sesuaikan jika perlu)\n",
        ")\n",
        "plt.title(\"SHAP Global Feature Importance (Rata-rata Absolute SHAP Value)\")\n",
        "plt.xlabel(\"mean(|SHAP value|)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ SHAP Global Feature Importance Bar Plot selesai.\")"
      ],
      "metadata": {
        "id": "UviJVVxEIO5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ SOLUSI CEPAT 1: PERMUTATION IMPORTANCE\n",
        "# =====================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# --- Pastikan variabel ini sudah ada ---\n",
        "# stack_model: Model yang sudah di-fit\n",
        "# X_res, y_res: Data yang digunakan untuk fit\n",
        "# feature_names: Daftar nama kolom\n",
        "# ----------------------------------------\n",
        "\n",
        "print(\"\\n--- Menghitung Permutation Importance (Jauh Lebih Cepat) ---\")\n",
        "\n",
        "# Kita hitung importance pada data training (X_res)\n",
        "# n_repeats=5 sudah cukup cepat dan stabil\n",
        "perm_importance = permutation_importance(\n",
        "    stack_model,\n",
        "    X_res,\n",
        "    y_res,\n",
        "    n_repeats=5,  # Ulang 5x (lebih cepat dari 10x)\n",
        "    random_state=42,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Urutkan fitur berdasarkan importance\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "\n",
        "# Buat DataFrame untuk visualisasi\n",
        "perm_df = pd.DataFrame(\n",
        "    data={\n",
        "        'feature': np.array(feature_names)[sorted_idx],\n",
        "        'importance_mean': perm_importance.importances_mean[sorted_idx],\n",
        "    }\n",
        ").sort_values(by='importance_mean', ascending=False)\n",
        "\n",
        "print(\"\\nFitur Paling Berpengaruh (Permutation Importance):\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "print(perm_df)\n",
        "pd.reset_option('display.max_rows')\n",
        "\n",
        "# --- Visualisasi Bar Plot ---\n",
        "# Atur tinggi plot secara dinamis\n",
        "num_features = len(perm_df)\n",
        "plot_height = max(8, num_features * 0.4)\n",
        "\n",
        "plt.figure(figsize=(12, plot_height))\n",
        "sns.barplot(\n",
        "    data=perm_df,\n",
        "    x='importance_mean',\n",
        "    y='feature',\n",
        "    palette='rocket'\n",
        ")\n",
        "plt.title(\"Global Feature Importance (Permutation Importance - Fast)\")\n",
        "plt.xlabel(\"Penurunan Rata-rata Skor AUC (Semakin tinggi semakin penting)\")\n",
        "plt.ylabel(\"Fitur\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_BbmrPOLRy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ BLOK KODE UNTUK BEESWARM PLOT (Metode \"Proxy\" Cepat)\n",
        "# =====================================================\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Pastikan variabel ini sudah ada dari script utama Anda ---\n",
        "# stack_model: Model StackingClassifier Anda yang sudah di-fit\n",
        "# X_res: DataFrame fitur Anda (misalnya, X_res)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== SHAP BEESWARM PLOTS (Metode \\\"Proxy\\\" Cepat) ===\")\n",
        "shap.initjs() # Inisialisasi javascript untuk plotting\n",
        "\n",
        "# Ambil sampel data. 2000 sampel sudah lebih dari cukup\n",
        "# untuk beeswarm plot yang representatif.\n",
        "sample_data_beeswarm = shap.sample(X_res, 2000, random_state=42)\n",
        "\n",
        "# Loop ketiga base model yang sudah di-fit di dalam stack\n",
        "for model_name, _ in stack_model.estimators:\n",
        "    try:\n",
        "        print(f\"\\n--- Menganalisis SHAP Beeswarm untuk: {model_name} ---\")\n",
        "\n",
        "        # 1. Akses model yang sudah di-fit dari stack\n",
        "        model_in_stack = stack_model.named_estimators_[model_name]\n",
        "\n",
        "        # 2. Gunakan shap.TreeExplainer (CEPAT)\n",
        "        explainer_fast = shap.TreeExplainer(model_in_stack)\n",
        "\n",
        "        # 3. Hitung SHAP values\n",
        "        shap_values_fast = explainer_fast(sample_data_beeswarm)\n",
        "\n",
        "        # 4. Buat Summary Plot (Beeswarm)\n",
        "        #    plot_type=\"dot\" adalah default, jadi kita bisa hilangkan.\n",
        "\n",
        "        try:\n",
        "            # Coba ambil shap values untuk kelas 1 (Attrition)\n",
        "            shap_values_for_plot = shap_values_fast.values[:,:,1]\n",
        "            data_for_plot = sample_data_beeswarm\n",
        "        except IndexError:\n",
        "            # Jika output hanya 1 kelas (jarang terjadi di klasifikasi)\n",
        "            shap_values_for_plot = shap_values_fast.values\n",
        "            data_for_plot = sample_data_beeswarm\n",
        "\n",
        "        print(f\"Membuat plot untuk {model_name}...\")\n",
        "        shap.summary_plot(\n",
        "            shap_values_for_plot,\n",
        "            data_for_plot,\n",
        "            plot_type=\"dot\", # \"dot\" adalah beeswarm plot\n",
        "            show=False,\n",
        "            max_display=20 # Tampilkan 20 fitur teratas\n",
        "        )\n",
        "        plt.title(f\"SHAP Beeswarm Plot untuk: {model_name} (Kelas Attrition=1)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat membuat SHAP Beeswarm untuk {model_name}: {e}\")\n",
        "\n",
        "print(\"\\n✅ SHAP Beeswarm Plots (Proxy) selesai.\")"
      ],
      "metadata": {
        "id": "2v0LSnurL9F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ✅ BLOK KODE: ESTIMASI DAMPAK FINANSIAL (FINANCIAL RESULT)\n",
        "# =====================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"✅ MEMULAI ANALISIS DAMPAK FINANSIAL (FINANCIAL RESULT)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- 1. Definisikan Asumsi Dasar ---\n",
        "# Mirip dengan 'biaya 18%' dan 'diskon 8%' Anda, kita tetapkan asumsi biaya:\n",
        "# Asumsi: Biaya turnover (kehilangan karyawan) setara dengan 6 bulan gaji.\n",
        "# Asumsi: Biaya retensi (bonus/kenaikan gaji) setara dengan 1 bulan gaji.\n",
        "\n",
        "ASSUMPTIONS = {\n",
        "    'cost_turnover_months': 6.0,  # Biaya jika FN (False Negative)\n",
        "    'cost_retention_months': 1.0  # Biaya jika FP (False Positive)\n",
        "}\n",
        "\n",
        "# Benefit (TP) = Biaya yang Dihindari - Biaya yang Dikeluarkan\n",
        "# Benefit (TP) = (6 bulan gaji) - (1 bulan gaji) = 5 bulan gaji\n",
        "ASSUMPTIONS['benefit_retention_months'] = ASSUMPTIONS['cost_turnover_months'] - ASSUMPTIONS['cost_retention_months']\n",
        "\n",
        "print(\"Asumsi Finansial (berdasarkan Gaji Bulanan):\")\n",
        "print(f\"  - Biaya Turnover (FN): {ASSUMPTIONS['cost_turnover_months']:.1f}x Gaji Bulanan\")\n",
        "print(f\"  - Biaya Retensi (FP):  {ASSUMPTIONS['cost_retention_months']:.1f}x Gaji Bulanan\")\n",
        "print(f\"  - Benefit Retensi (TP): {ASSUMPTIONS['benefit_retention_months']:.1f}x Gaji Bulanan\")\n",
        "print(f\"  - Biaya/Benefit (TN): 0.0x Gaji Bulanan (Baseline)\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Siapkan Data Aktual (Bukan Data SMOTE) ---\n",
        "# Penting: Analisis finansial harus menggunakan data asli, bukan data sintetis.\n",
        "# Kita gunakan model yang dilatih pada 'X_res' untuk memprediksi 'X' (data asli).\n",
        "# 'X' adalah DataFrame fitur Anda yang sudah di-encode dan di-scale.\n",
        "# 'y' adalah target 'Attrition' asli Anda.\n",
        "# 'train' adalah DataFrame asli sebelum diubah, untuk mengambil 'MonthlyIncome'.\n",
        "\n",
        "print(\"Mendapatkan prediksi probabilitas pada data *asli*...\")\n",
        "try:\n",
        "    # Dapatkan probabilitas prediksi dari model akhir pada data asli 'X'\n",
        "    y_probs_actual = stack_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Buat DataFrame hasil untuk analisis\n",
        "    results_df = pd.DataFrame({\n",
        "        'Actual_Attrition': y,\n",
        "        'Prob_Attrition': y_probs_actual,\n",
        "        'MonthlyIncome': train['MonthlyIncome'] # Ambil pendapatan asli\n",
        "    })\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saat membuat prediksi pada data 'X'. Pastikan 'X' dan 'train' tersedia. Error: {e}\")\n",
        "    # Hentikan eksekusi jika data penting tidak ada\n",
        "    raise\n",
        "\n",
        "# --- 3. Fungsi Kalkulator Finansial ---\n",
        "# Fungsi ini akan menghitung total net benefit berdasarkan 'threshold' probabilitas\n",
        "def calculate_financial_impact(df, assumptions, threshold):\n",
        "\n",
        "    # Tentukan prediksi (1 atau 0) berdasarkan threshold\n",
        "    df['Predicted_Attrition'] = (df['Prob_Attrition'] > threshold).astype(int)\n",
        "\n",
        "    # Pisahkan berdasarkan Confusion Matrix (TP, FP, FN, TN)\n",
        "    tp_df = df[(df['Actual_Attrition'] == 1) & (df['Predicted_Attrition'] == 1)]\n",
        "    fp_df = df[(df['Actual_Attrition'] == 0) & (df['Predicted_Attrition'] == 1)]\n",
        "    fn_df = df[(df['Actual_Attrition'] == 1) & (df['Predicted_Attrition'] == 0)]\n",
        "    tn_df = df[(df['Actual_Attrition'] == 0) & (df['Predicted_Attrition'] == 0)]\n",
        "\n",
        "    # Hitung total dampak finansial untuk setiap kuadran\n",
        "\n",
        "    # True Positive (TP): Karyawan diprediksi keluar, DIBERI retensi, dan berhasil (asumsi).\n",
        "    # Benefit: Menghindari biaya turnover, dikurangi biaya retensi.\n",
        "    total_benefit_tp = (tp_df['MonthlyIncome'] * assumptions['benefit_retention_months']).sum()\n",
        "\n",
        "    # False Positive (FP): Karyawan diprediksi keluar, DIBERI retensi, padahal TIDAK akan keluar.\n",
        "    # Cost: Biaya retensi yang tidak perlu.\n",
        "    total_cost_fp = (fp_df['MonthlyIncome'] * assumptions['cost_retention_months']).sum()\n",
        "\n",
        "    # False Negative (FN): Karyawan diprediksi bertahan, TIDAK diberi retensi, dan ternyata KELUAR.\n",
        "    # Cost: Kehilangan pendapatan (biaya turnover penuh).\n",
        "    total_cost_fn = (fn_df['MonthlyIncome'] * assumptions['cost_turnover_months']).sum()\n",
        "\n",
        "    # True Negative (TN): Karyawan diprediksi bertahan, TIDAK diberi retensi, dan memang bertahan.\n",
        "    # Cost/Benefit: 0 (ini adalah baseline kita).\n",
        "    total_benefit_tn = 0\n",
        "\n",
        "    # Hitung total Net Financial Impact\n",
        "    net_impact = total_benefit_tp - total_cost_fp - total_cost_fn\n",
        "\n",
        "    return net_impact, len(tp_df), len(fp_df), len(fn_df)\n",
        "\n",
        "# --- 4. Hitung Dampak pada Threshold Default (0.5) ---\n",
        "default_threshold = 0.5\n",
        "net_impact_default, tp, fp, fn = calculate_financial_impact(results_df.copy(), ASSUMPTIONS, default_threshold)\n",
        "\n",
        "print(f\"--- Hasil Finansial (Default Threshold = {default_threshold}) ---\")\n",
        "print(f\"  Estimasi Total Net Benefit: ${net_impact_default:,.2f}\")\n",
        "print(f\"  Intervensi dilakukan pada: {tp+fp} karyawan (TP + FP)\")\n",
        "print(f\"  Karyawan yang keluar & gagal dicegah (FN): {fn} karyawan\\n\")\n",
        "\n",
        "# --- 5. Optimasi Threshold untuk Keuntungan Maksimal ---\n",
        "# Ini adalah implementasi dari \"menargetkan pelanggan dengan probabilitas churn lebih tinggi\"\n",
        "print(\"Mencari threshold probabilitas optimal untuk Net Benefit maksimal...\")\n",
        "thresholds = np.linspace(0.05, 0.95, 100) # Cek 100 threshold berbeda\n",
        "financial_results = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    net_impact, _, _, _ = calculate_financial_impact(results_df.copy(), ASSUMPTIONS, thresh)\n",
        "    financial_results.append({\n",
        "        'threshold': thresh,\n",
        "        'net_benefit': net_impact\n",
        "    })\n",
        "\n",
        "# Konversi ke DataFrame untuk analisis mudah\n",
        "financial_df = pd.DataFrame(financial_results)\n",
        "\n",
        "# Cari hasil terbaik\n",
        "best_result = financial_df.loc[financial_df['net_benefit'].idxmax()]\n",
        "optimal_threshold = best_result['threshold']\n",
        "max_benefit = best_result['net_benefit']\n",
        "\n",
        "print(f\"\\n--- Hasil Finansial (Optimal Threshold = {optimal_threshold:.2f}) ---\")\n",
        "print(f\"  Estimasi Total Net Benefit Maksimal: ${max_benefit:,.2f}\")\n",
        "\n",
        "# Hitung ulang jumlah TP/FP/FN pada threshold optimal\n",
        "_, tp_opt, fp_opt, fn_opt = calculate_financial_impact(results_df.copy(), ASSUMPTIONS, optimal_threshold)\n",
        "print(f\"  Intervensi dilakukan pada: {tp_opt+fp_opt} karyawan (TP + FP)\")\n",
        "print(f\"  Karyawan yang keluar & gagal dicegah (FN): {fn_opt} karyawan\")\n",
        "print(\"\\nAnalisis ini menunjukkan bahwa dengan memilih threshold secara strategis,\")\n",
        "print(\"perusahaan dapat memaksimalkan penghematan biaya.\")\n",
        "print(\"Threshold yang lebih tinggi = lebih 'konservatif', mengurangi biaya FP.\")\n",
        "\n",
        "\n",
        "# --- 6. Visualisasi Hasil Finansial vs. Threshold ---\n",
        "print(\"\\nMembuat plot 'Net Benefit vs. Threshold'...\")\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.lineplot(data=financial_df, x='threshold', y='net_benefit', linewidth=2.5)\n",
        "plt.axvline(x=optimal_threshold, color='red', linestyle='--',\n",
        "            label=f'Optimal Threshold ({optimal_threshold:.2f})\\nMax Benefit: ${max_benefit:,.0f}')\n",
        "plt.axvline(x=default_threshold, color='gray', linestyle=':',\n",
        "            label=f'Default Threshold (0.50)\\nBenefit: ${net_impact_default:,.0f}')\n",
        "plt.title('Estimasi Net Benefit vs. Threshold Prediksi Attrition', fontsize=16)\n",
        "plt.xlabel('Threshold Probabilitas untuk Intervensi', fontsize=12)\n",
        "plt.ylabel('Total Estimasi Net Benefit ($)', fontsize=12)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, linestyle=':', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🎯 Analisis Dampak Finansial selesai!\")\n",
        "# ====================================================="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "MfNN99tldgYV",
        "outputId": "285b25e6-83a0-4f99-8a43-64ca11240069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "✅ MEMULAI ANALISIS DAMPAK FINANSIAL (FINANCIAL RESULT)\n",
            "==================================================\n",
            "Asumsi Finansial (berdasarkan Gaji Bulanan):\n",
            "  - Biaya Turnover (FN): 6.0x Gaji Bulanan\n",
            "  - Biaya Retensi (FP):  1.0x Gaji Bulanan\n",
            "  - Benefit Retensi (TP): 5.0x Gaji Bulanan\n",
            "  - Biaya/Benefit (TN): 0.0x Gaji Bulanan (Baseline)\n",
            "\n",
            "Mendapatkan prediksi probabilitas pada data *asli*...\n",
            "Error saat membuat prediksi pada data 'X'. Pastikan 'X' dan 'train' tersedia. Error: name 'stack_model' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stack_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1962520615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Dapatkan probabilitas prediksi dari model akhir pada data asli 'X'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0my_probs_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Buat DataFrame hasil untuk analisis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stack_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMd1j70kg8yP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}